{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422e5404-ab57-49c2-a7ef-10e70d69241e",
   "metadata": {},
   "source": [
    "To complete the sentiment analysis task, I used the keras nlp BERT model to train the dataset in order to predict the sentiment of the phrase in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12324054-5892-42c8-b848-52e59e27baaa",
   "metadata": {},
   "source": [
    "To begin with, I had to install all the packages needed for the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c1562a-2b76-4cda-adbb-ef6393856dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q keras-nlp\n",
    "!pip install -q keras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aef02fb9-9eab-48d8-a3ba-224fadedf031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow[and-cuda] in /home/emily4664/.local/lib/python3.10/site-packages (2.15.0.post1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow[and-cuda]) (59.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.15.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (4.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.14.1)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.15.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (23.1)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (23.5.26)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.25.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.10.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.34.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (16.0.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.59.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow[and-cuda]) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/emily4664/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.15.1)\n",
      "Collecting nvidia-cublas-cu12==12.2.5.6\n",
      "  Downloading nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl (417.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.8/417.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.2.141\n",
      "  Downloading nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl (195.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.3/195.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.2.142\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.16.5\n",
      "  Downloading nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.5.2.141\n",
      "  Downloading nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl (124.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.2.140\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (23.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.4.25\n",
      "  Downloading nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl (720.1 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m504.2/720.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:33\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/urllib3/response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 466, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/lib/python3.10/ssl.py\", line 1274, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/lib/python3.10/ssl.py\", line 1130, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/base_command.py\", line 165, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/req_command.py\", line 205, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/commands/install.py\", line 339, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 373, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 213, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_updated_criteria(candidate)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 204, in _get_updated_criteria\n",
      "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 215, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 288, in __init__\n",
      "    super().__init__(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 227, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 299, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 487, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 532, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 214, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 94, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/network/download.py\", line 146, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/progress_bars.py\", line 304, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/urllib3/response.py\", line 512, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8aad4ddc-e927-4dd7-a59b-7aa75c118cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/emily4664/.local/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: tqdm, safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.1 huggingface-hub-0.19.4 safetensors-0.4.1 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76d36022-f92e-4afa-9b01-767d6ffc7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikeras\n",
      "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/emily4664/.local/lib/python3.10/site-packages (from scikeras) (1.3.2)\n",
      "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/emily4664/.local/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/emily4664/.local/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/emily4664/.local/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (1.11.3)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7236c4-1e33-47f1-a388-750edd533130",
   "metadata": {},
   "source": [
    "Then I imported all the modules needed so that it could be used and then opened up the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10a966ca-d41c-4387-8673-52619cc5223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "730aa877-bfbd-42d9-ade5-891c07c73449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66287</th>\n",
       "      <td>222348</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66288</th>\n",
       "      <td>222349</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66289</th>\n",
       "      <td>222350</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66290</th>\n",
       "      <td>222351</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66291</th>\n",
       "      <td>222352</td>\n",
       "      <td>11855</td>\n",
       "      <td>predictable scenario</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66292 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PhraseId  SentenceId                                             Phrase\n",
       "0        156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1        156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2        156063        8545                                                 An\n",
       "3        156064        8545  intermittently pleasing but mostly routine effort\n",
       "4        156065        8545         intermittently pleasing but mostly routine\n",
       "...         ...         ...                                                ...\n",
       "66287    222348       11855             A long-winded , predictable scenario .\n",
       "66288    222349       11855               A long-winded , predictable scenario\n",
       "66289    222350       11855                                    A long-winded ,\n",
       "66290    222351       11855                                      A long-winded\n",
       "66291    222352       11855                               predictable scenario\n",
       "\n",
       "[66292 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('test.tsv', delimiter='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbfcda3-c10c-46e5-a80b-32542e6be9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>156056</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>156057</td>\n",
       "      <td>8544</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>156059</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156060 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "0              1           1   \n",
       "1              2           1   \n",
       "2              3           1   \n",
       "3              4           1   \n",
       "4              5           1   \n",
       "...          ...         ...   \n",
       "156055    156056        8544   \n",
       "156056    156057        8544   \n",
       "156057    156058        8544   \n",
       "156058    156059        8544   \n",
       "156059    156060        8544   \n",
       "\n",
       "                                                   Phrase  Sentiment  \n",
       "0       A series of escapades demonstrating the adage ...          1  \n",
       "1       A series of escapades demonstrating the adage ...          2  \n",
       "2                                                A series          2  \n",
       "3                                                       A          2  \n",
       "4                                                  series          2  \n",
       "...                                                   ...        ...  \n",
       "156055                                          Hearst 's          2  \n",
       "156056                          forced avuncular chortles          1  \n",
       "156057                                 avuncular chortles          3  \n",
       "156058                                          avuncular          2  \n",
       "156059                                           chortles          2  \n",
       "\n",
       "[156060 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('train.tsv', delimiter='\\t')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71969f7-ec4d-4e97-a5e2-b3fe761a63ce",
   "metadata": {},
   "source": [
    "The dataset was too large and had too many instances, which caused the kernal to die. To fix this I decreased the data subset so that it could run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1620055-2861-4b5d-a9d2-6cc3e23f3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 1000\n",
    "subset_data = data.sample(n=subset_size, random_state=42)\n",
    "\n",
    "fraction_to_use = 0.1  \n",
    "subset_data = data.sample(frac=fraction_to_use, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494840d-52fa-4a5e-bfcf-af7c834c9f07",
   "metadata": {},
   "source": [
    "I created directories for each of the 5 sentiment labels and then sorted the phrase instance into a dirctory based on its corresponding sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c538c5-d378-4031-8ee4-ff805b7c10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentiment in range(5):\n",
    "    directory = str(sentiment)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    phrase_id = row['PhraseId']\n",
    "    sentiment = row['Sentiment']\n",
    "    content = row['Phrase']\n",
    "\n",
    "    filename = f\"{phrase_id}.txt\"\n",
    "    directory = str(sentiment)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce13f71-ff9c-4b5d-a866-ea751c10079b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0' created successfully.\n",
      "Directory '1' created successfully.\n",
      "Directory '2' created successfully.\n",
      "Directory '3' created successfully.\n",
      "Directory '4' created successfully.\n"
     ]
    }
   ],
   "source": [
    "for sentiment in range(5):\n",
    "    directory = str(sentiment)\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Error creating directory '{directory}'.\")\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    phrase_id = row['PhraseId']\n",
    "    sentiment = row['Sentiment']\n",
    "    content = row['Phrase']\n",
    "\n",
    "    filename = f\"{phrase_id}.txt\"\n",
    "    directory = str(sentiment)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7fff5c-57e1-41f2-a922-c95fcc23f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 156062 files belonging to 5 classes.\n",
      "Using 124850 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 16:10:16.246486: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-06 16:10:16.246905: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "directory = \"/home/emily4664/SentimentAnalysis/notebooks/main directory\"\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59494c44-beda-47af-90e6-67e63dc80d51",
   "metadata": {},
   "source": [
    "To start the process of training, the phrases needed to be tokenized so that the machine can read it more easily for analysis. The tokenization was done using the keras BERT tokenizer, which I initialized below. I also loaded the prebuilt BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cbbced5-fed5-4c6f-b2f0-95302bb4b63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 16:24:18.132875: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-06 16:24:18.133269: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd33be-6e84-4655-8a49-788afaf1c38a",
   "metadata": {},
   "source": [
    "The training dataset was then tokenized and trained using the keras BERT model here which was also compiled with the adam optimizer, spare cross entropy loss function, and accuracy metric as well as fitted for three epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b986b73d-d6aa-48c6-bc5d-c77b4e7d4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(data):\n",
    "    return tokenizer(data, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "text_batch = subset_data['Phrase'].tolist()\n",
    "tokenized_batch = tokenize_reviews(text_batch) \n",
    "labels = subset_data['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1bdf1f-062f-4b43-93bf-03c51ee7cc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "488/488 [==============================] - 2312s 5s/step - loss: 1.3325 - accuracy: 0.4971\n",
      "Epoch 2/3\n",
      "488/488 [==============================] - 2331s 5s/step - loss: 1.3110 - accuracy: 0.5003\n",
      "Epoch 3/3\n",
      "488/488 [==============================] - 2370s 5s/step - loss: 1.3067 - accuracy: 0.5003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7f087c7400>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "token_type_ids = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "attention_mask = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "bert_outputs = bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).logits\n",
    "output = keras.layers.Dense(5, activation='softmax')(bert_outputs)\n",
    "\n",
    "model = keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    {\"input_ids\": tokenized_batch['input_ids'],\n",
    "     \"token_type_ids\": tokenized_batch['token_type_ids'],\n",
    "     \"attention_mask\": tokenized_batch['attention_mask']},\n",
    "    labels,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92133041-adb0-45e0-a61f-a383a12e551d",
   "metadata": {},
   "source": [
    "The test dataset was tokenized and trained next with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25e06ef3-5a72-4bf3-b32c-fc2eac5e2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_test = df['Phrase'].tolist()\n",
    "text_data_test = [str(phrase) for phrase in text_data_test]\n",
    "try:\n",
    "    tokenized_test_batch = tokenizer(\n",
    "        text_data_test,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error during tokenization:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb723881-6a88-459f-940d-171d3cba6015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2072/2072 [==============================] - 3131s 2s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(\n",
    "    {\"input_ids\": tokenized_test_batch['input_ids'],\n",
    "     \"token_type_ids\": tokenized_test_batch['token_type_ids'],\n",
    "     \"attention_mask\": tokenized_test_batch['attention_mask']}\n",
    ")\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f881d10c-af87-4633-848c-3c35ba172ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels for Test Data: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Labels for Test Data:\", predicted_labels[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df84ab8-1391-4e21-b862-b5642db12ca4",
   "metadata": {},
   "source": [
    "Right off the bat, the output does not look very accurate as it has assigned a sentiment of 2 to all the phrases. The decreased accuracy may be explained by the fact that the training dataset has a majority sentiment of 2 to begin with, and when I further cut down the training data subset, there was very few instances of other sentiments. It seems that the model just fit everything as 2 because of that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
